<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="https://cdn.tailwindcss.com" rel="stylesheet">
    <link rel="stylesheet" href="styles.css">
    <title>3: Historical Timeline of Machine Learning</title>
</head>

<body>
    <header class="bg-gray-800 text-white p-4">
        <nav>
            <ul class="flex space-x-4">
                <li><a href="index.html" class="text-blue-400">Home</a></li>
                <li><a href="lecture1.html" class="text-blue-400">1</a></li>
                <li><a href="lecture2.html" class="text-blue-400">2</a></li>
                <li><a href="lecture4.html" class="text-blue-400">4</a></li>
                <li><a href="lecture5.html" class="text-blue-400">5</a></li>
                <li><a href="lecture6.html" class="text-blue-400">6</a></li>
                <li><a href="lecture7.html" class="text-blue-400">7</a></li>
                <li><a href="lecture8.html" class="text-blue-400">8</a></li>
            </ul>
        </nav>
        <h1>3: Historical Timeline of Machine Learning</h1>
    </header>

    <main class="p-4">
        <section>
            <p>Explore the key milestones in the evolution of <a href="https://en.wikipedia.org/wiki/Machine_learning" class="text-blue-400">machine learning</a> from its inception to the present day.</p>

            <h3>Timeline of Key Developments</h3>
            <ul>
                <li><strong>1950s</strong> - Early Foundations:
                    <ul>
                        <li>1950: <a href="https://en.wikipedia.org/wiki/Turing_test" class="text-blue-400">Alan Turing</a> publishes "Computing Machinery and Intelligence," proposing what is now called the <a href="https://en.wikipedia.org/wiki/Turing_test" class="text-blue-400">Turing Test</a>.</li>
                        <li>1952: <a href="https://en.wikipedia.org/wiki/Arthur_Samuel" class="text-blue-400">Arthur Samuel</a> develops the first computer learning program, which was a game of checkers.</li>
                    </ul>
                </li>
                <img src="./perceptron-1961.png" alt="Perceptron" class="section-image">

                <li><strong>1960s</strong> - Concept of Neural Networks:
                    <ul>
                        <li>1961: <a href="https://en.wikipedia.org/wiki/Frank_Rosenblatt" class="text-blue-400">Frank Rosenblatt</a> proposes the <a href="https://en.wikipedia.org/wiki/Perceptron" class="text-blue-400">Perceptron</a>, an early neural network.</li>
                        <li>1967: The <a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm" class="text-blue-400">nearest neighbor algorithm</a> is written, allowing computers to begin using basic pattern recognition.</li>
                    </ul>
                    <p><a href="perceptron.html" class="text-blue-400">The Perceptron Explained</a></p>
                </li>
                <p></p>
                <li><strong>1980s</strong> - Revival and Expansion:
                    <ul>
                        <li>1980: <a href="https://en.wikipedia.org/wiki/Geoffrey_Hinton" class="text-blue-400">Geoffrey Hinton</a> and others popularize the concept of <a href="https://en.wikipedia.org/wiki/Backpropagation" class="text-blue-400">backpropagation</a>, improving training for multi-layer networks.</li>
                        <li>1985: <a href="https://en.wikipedia.org/wiki/Terry_Sejnowski" class="text-blue-400">Terry Sejnowski</a> and <a href="https://en.wikipedia.org/wiki/Charles_Rosenberg" class="text-blue-400">Charles Rosenberg</a> develop <a href="https://en.wikipedia.org/wiki/NetTalk" class="text-blue-400">NetTalk</a>, which learns to pronounce English text aloud using a neural network.</li>
                    </ul>
                    <p><a href="https://www.youtube.com/watch?v=SmZmBKc7Lrs" class="text-blue-400">Video - The Most Important Algorithm in Machine Learning</a></p>
                    <p><a href="curve-fitting.html" class=" text-blue-400">Curve Fitting</a></p>
                    <img src="./NER.png" alt="NER" class="section-image">
                    <p><a href="email-filtering.html" class=" text-blue-400">Named Entity Recognition for Email Data Extraction</a></p>
                </li>
                <p></p>
                <li><strong>1990s</strong> - Support Vector Machines and Advances in Theory:
                    <ul>
                        <li>1995: <a href="https://en.wikipedia.org/wiki/Support_vector_machine" class="text-blue-400">Support Vector Machines (SVMs)</a> are developed, providing a new generation of learning algorithms.</li>
                        <li>1997: <a href="https://en.wikipedia.org/wiki/Long_short-term_memory" class="text-blue-400">Long Short-Term Memory (LSTM)</a> networks are introduced by <a href="https://en.wikipedia.org/wiki/Sepp_Hochreiter" class="text-blue-400">Hochreiter</a> & <a href="https://en.wikipedia.org/wiki/J%C3%BCrgen_Schmidhuber" class="text-blue-400">Schmidhuber</a>, significantly improving recurrent neural network capabilities.</li>
                    </ul>
                    <p><a href="lstm.html" class=" text-blue-400">Long Short-Term Memory (LSTM)</a></p>
                </li>
                <img src="./svms-1990s.png" alt="SVMs" class="section-image">

                <li><strong>2000s</strong> - The Era of Big Data:
                    <ul>
                        <li>2006: <a href="https://en.wikipedia.org/wiki/Geoffrey_Hinton" class="text-blue-400">Geoffrey Hinton</a> et al. introduce concepts of <a href="https://en.wikipedia.org/wiki/Deep_learning" class="text-blue-400">deep learning</a> in neural networks with deep architectures.</li>
                        <li>2009: <a href="https://en.wikipedia.org/wiki/Fei-Fei_Li" class="text-blue-400">Fei-Fei Li</a> launches <a href="https://en.wikipedia.org/wiki/ImageNet" class="text-blue-400">ImageNet</a>, a large visual database designed for use in visual object recognition software research.</li>
                    </ul>
                    <p><a href="deep-learning-2006.html" text-blue-400">Deep Learning</a></p>
                </li>

                <li><strong>2010s</strong> - Deep Learning Breakthroughs:
                    <ul>
                        <li>2012: <a href="https://en.wikipedia.org/wiki/AlexNet" class="text-blue-400">AlexNet</a> wins the ImageNet challenge, significantly outperforming the second-place competitor, catalyzing the focus on deep learning in the AI community.</li>
                        <li>2016: Google's <a href="https://en.wikipedia.org/wiki/AlphaGo" class="text-blue-400">AlphaGo</a> defeats <a href="https://en.wikipedia.org/wiki/Lee_Sedol" class="text-blue-400">Lee Sedol</a> in Go, a milestone for AI in complex problem-solving.</li>
                    </ul>
                    
                </li>
                <p><a href="convolutional-neural-networks.html" text-blue-400">Convolutional Neural Networks</a></p>
                <li><strong>2017</strong> - Transformers Revolution:
                    <ul>
                        <li>2017: The paper "<a href="https://en.wikipedia.org/wiki/Attention_Is_All_You_Need" class="text-blue-400">Attention Is All You Need</a>" by <a href="https://en.wikipedia.org/wiki/Ashish_Vaswani" class="text-blue-400">Vaswani</a> et al. is published, introducing the <a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)" class="text-blue-400">transformer model</a>, which uses self-attention mechanisms to significantly improve the efficiency of training language models.</li>
                    </ul>
                    <p><a href="attention.html" text-blue-400">Attention is All You Need</a></p>
                    <p><a href="lecture7.html" text-blue-400">Transformer Architecture</a></p>
                </li>
                <img src="./transformer-2017.png" alt="Transformer" class="section-image">
                <p></p>
                <li><strong>2018</strong> - Advancements in Natural Language Processing:
                    <ul>
                        <li>2018: <a href="https://en.wikipedia.org/wiki/BERT_(language_model)" class="text-blue-400">BERT</a> (Bidirectional Encoder Representations from Transformers) is developed by researchers at Google, setting new standards for various NLP tasks, including question answering and language inference.</li>
                    </ul>
                </li>
                <p></p>
                <li><strong>2019</strong> - GPT-2 and the Rise of Generative Models:
                    <ul>
                        <li>2019: <a href="https://en.wikipedia.org/wiki/GPT-2" class="text-blue-400">OpenAI</a> introduces <a href="https://en.wikipedia.org/wiki/GPT-2" class="text-blue-400">GPT-2</a>, a large transformer-based language model known for its ability to generate coherent and contextually relevant text on a wide range of topics.</li>
                    </ul>
                </li>
                <p></p>
                <li><strong>2020</strong> - AI Ethics and Regulation:
                    <ul>
                        <li>2020: <a href="https://en.wikipedia.org/wiki/Artificial_intelligence_ethics" class="text-blue-400">AI regulation</a> begins to take shape with the European Union proposing the first legal framework for ethical AI use, focusing on transparency, accountability, and accuracy.</li>
                    </ul>
                </li>
                <p></p>
                <li><strong>2021</strong> - Foundation Models:
                    <ul>
                        <li>2021: The concept of <a href="https://en.wikipedia.org/wiki/Foundation_models" class="text-blue-400">foundation models</a> is proposed, characterized by large-scale models trained on broad data at scale that are adaptable to a wide range of tasks.</li>
                    </ul>
                </li>
                <p></p>
                <li><strong>2022</strong> - Advances in Multimodal AI:
                    <ul>
                        <li>2022: AI models that can understand and generate multiple forms of data (text, image, video) simultaneously become more prevalent, enhancing capabilities in applications from autonomous vehicles to content creation.</li>
                    </ul>
                </li>
                <img src="./personalized-2024.png" alt="Personalized" class="section-image">
                <p></p>
                <li><strong>2024</strong> - Personalized AI:
                    <ul>
                        <li>2024: Advancements in <a href="https://en.wikipedia.org/wiki/Personalized_AI" class="text-blue-400">personalized AI</a> systems provide customized health diagnostics, learning supports, and financial advice, making artificial intelligence more directly applicable to individual daily needs.</li>
                    </ul>
                </li>
            </ul>
        </section>
    </main>

    <footer class="bg-gray-800 text-white p-4">
        <nav>
            <ul class="flex space-x-4">
                <li><a href="index.html" class="text-blue-400">Home</a></li>
                <li><a href="lecture1.html" class="text-blue-400">1</a></li>
                <li><a href="lecture2.html" class="text-blue-400">2</a></li>
                <li><a href="lecture4.html" class="text-blue-400">4</a></li>
                <li><a href="lecture5.html" class="text-blue-400">5</a></li>
                <li><a href="lecture6.html" class="text-blue-400">6</a></li>
                <li><a href="lecture7.html" class="text-blue-400">7</a></li>
                <li><a href="lecture8.html" class="text-blue-400">8</a></li>
            </ul>
        </nav>
    </footer>
</body>

</html>
