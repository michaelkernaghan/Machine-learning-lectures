<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="https://cdn.tailwindcss.com" rel="stylesheet">
    <link rel="stylesheet" href="styles.css">
    <title>Exploration 7: Transformer Architecture</title>
</head>

<body>
    <header class="bg-gray-800 text-white p-4">
        <nav>
            <ul class="flex space-x-4">
                <li><a href="index.html" class="text-blue-400">Home</a></li>
                <li><a href="lecture2.html" class="text-blue-400">2</a></li>
                <li><a href="lecture3.html" class="text-blue-400">3</a></li>
                <li><a href="lecture4.html" class="text-blue-400">4</a></li>
                <li><a href="lecture5.html" class="text-blue-400">5</a></li>
                <li><a href="lecture6.html" class="text-blue-400">6</a></li>
                <li><a href="lecture8.html" class="text-blue-400">8</a></li>
            </ul>
        </nav>
        <h1>Transformer Architecture</h1>
    </header>
    <main class="p-4">
        <div class="flex flex-col items-center">
            <img src="./transformer-architecture.png" alt="Transformer Architecture Diagram" class="w-full md:w-1/2">
            <div class="mt-4">
                <h2 class="text-xl font-bold">Transformer Model Architecture</h2>
                <p class="mt-2">This diagram represents the architecture of a Transformer model, which is a type of deep
                    learning model commonly used for natural language processing tasks. Here is a detailed description
                    of each step:</p>
                <ol class="list-decimal mt-4 space-y-4">
                    <li><strong>Input Embedding:</strong> The raw input tokens (words) are converted into vectors of
                        fixed size, known as embeddings. This step maps each word to a high-dimensional space where
                        words with similar meanings are closer together.</li>
                    <li><strong>Positional Encoding:</strong> Since the Transformer model does not have a sense of word
                        order, positional encodings are added to the input embeddings to provide information about the
                        position of each word in the sequence. These encodings are added element-wise to the input
                        embeddings.</li>
                    <li><strong>Add & Norm:</strong> This step involves adding the input (either embeddings or the
                        output of the previous layer) to the output of a sub-layer (such as Multi-Head Attention or Feed
                        Forward) and then normalizing the result. This helps stabilize and speed up the training
                        process.</li>
                    <li><strong>Multi-Head Attention:</strong> This component allows the model to focus on different
                        parts of the input sequence simultaneously. It uses multiple attention heads to capture
                        different aspects of the relationships between words in the sequence. Each head performs an
                        attention operation, and their outputs are concatenated and linearly transformed.</li>
                    <li><strong>Feed Forward:</strong> A fully connected feed-forward neural network is applied to each
                        position separately and identically. This consists of two linear transformations with a ReLU
                        activation in between.</li>
                    <li><strong>Add & Norm:</strong> Similar to the previous Add & Norm step, the output of the
                        feed-forward network is added to the input and normalized.</li>
                    <li><strong>Masked Multi-Head Attention (Decoder Only):</strong> In the decoder, masked multi-head
                        attention is used to ensure that each position can only attend to previous positions in the
                        output sequence. This prevents the model from seeing future tokens during training, which is
                        crucial for autoregressive generation.</li>
                    <li><strong>Add & Norm:</strong> The same Add & Norm step is applied after the masked multi-head
                        attention.</li>
                    <li><strong>Multi-Head Attention (Decoder Only):</strong> The decoder also includes a regular
                        multi-head attention layer, but it attends to the encoder's output sequence. This helps the
                        decoder to generate the output sequence based on the encoded input sequence.</li>
                    <li><strong>Add & Norm:</strong> Again, the Add & Norm step is applied after the multi-head
                        attention layer.</li>
                    <li><strong>Feed Forward (Decoder Only):</strong> The decoder includes a feed-forward neural network
                        similar to the one in the encoder.</li>
                    <li><strong>Add & Norm:</strong> The final Add & Norm step is applied after the feed-forward network
                        in the decoder.</li>
                    <li><strong>Linear & Softmax:</strong> The output of the last Add & Norm layer is fed into a linear
                        transformation followed by a softmax function. The linear transformation maps the output to the
                        vocabulary size, and the softmax function converts the scores into probabilities for each token
                        in the vocabulary.</li>
                    <li><strong>Output Probabilities:</strong> The final output is a probability distribution over the
                        vocabulary for each position in the output sequence, indicating the likelihood of each token
                        being the correct next token in the sequence.</li>
                </ol>
            </div>
        </div>
        <section>
            <h1>Dog Name Generator Using GPT Architecture</h1>
            <img src="./dog-name-gpt.png" alt="Generate Dog Names by training a model with GPT" class="section-image">
            <p>This
                <a href="dog-names-GPT.html" class="text-blue-400 hover:text-blue-700" target="_blank">webpage</a>
                demonstrates constructing a Generative Pre-Trained Transformer to generate dog names.
            </p>
            The full Python script can be found in the GitHub repository:
            <a href="https://github.com/michaelkernaghan/Machine-learning-lectures/blob/main/python/model-training/dog-name-GPT.py"
                class="text-blue-400">dog-name-GPT.py</a>
        </section>
    </main>
    <footer>
        <nav>
            <ul class="bg-gray-800 text-white p-4">
                <li><a href="index.html" class="text-blue-400">Home</a></li>
                <li><a href="lecture2.html" class="text-blue-400">2</a></li>
                <li><a href="lecture3.html" class="text-blue-400">3</a></li>
                <li><a href="lecture4.html" class="text-blue-400">4</a></li>
                <li><a href="lecture5.html" class="text-blue-400">5</a></li>
                <li><a href="lecture6.html" class="text-blue-400">6</a></li>
                <li><a href="lecture8.html" class="text-blue-400">8</a></li>
            </ul>
        </nav>
    </footer>
</body>

</html>