<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="https://cdn.tailwindcss.com" rel="stylesheet">
    <link rel="stylesheet" href="./styles.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
   
    <title>Understanding the Perceptron</title>

    <style>
        table {
            width: 100%;
            border-collapse: collapse;
        }

        th,
        td {
            border: 1px solid black;
            padding: 8px;
            text-align: left;
        }

        th {
            background-color: #f2f2f2;
        }

        img {
            width: 100%;
            height: auto;
            margin-top: 20px;
        }

        pre {
            background-color: #f4f4f4;
            border: 1px solid #ddd;
            border-left: 3px solid #f36d33;
            color: #666;
            page-break-inside: avoid;
            font-family: monospace;
            font-size: 15px;
            line-height: 1.6;
            margin-bottom: 1.6em;
            max-width: 100%;
            overflow: auto;
            padding: 1em 1.5em;
            display: block;
            word-wrap: break-word;
        }

        code {
            color: #555;
            word-break: normal;
        }
    </style>
</head>

<body>
    <header class="bg-gray-800 text-white p-4">
        <nav>
            <ul class="flex space-x-4">
                <li><a href="index.html" class="text-blue-400">Home</a></li>
                <li><a href="perceptron.html" class="text-blue-400">Perceptron</a></li>
                <li><a href="applications.html" class="text-blue-400">Applications</a></li>
            </ul>
        </nav>
        <h1>Understanding the Perceptron</h1>
    </header>
    <main class="p-4">
        <section>
            <h2>Introduction to Perceptrons</h2>
            <p>Perceptrons are the simplest type of artificial neural network, often used as a stepping stone to understand more complex network structures. This section delves into the perceptron's basics, its architecture, and its role as a linear classifier.</p>
            <img src="images/perceptron.png" alt="Perceptron Architecture">

            <h3>Architecture of a Perceptron</h3>
            <p>The perceptron consists of input nodes (or neurons) connected to a single output node via weighted edges. The output node applies an activation function to the weighted sum of the inputs to produce the output.</p>

            <img src="images/perceptron2.png" alt="Perceptron Training Process">
            <h3>Training a Perceptron</h3>
            <p>Training a perceptron involves adjusting the weights based on the errors made by the model during the training process. This is typically done using the perceptron learning rule, which is a type of online learning.</p>

            <img src="images/perceptron3.png" alt="Perceptron Example">
            <h3>Example Applications of Perceptrons</h3>
            <section>
                <h2>Perceptrons for Binary Classification</h2>

                <h3>Concept</h3>
                <p>A perceptron is a type of artificial neuron that takes several binary inputs, applies a set of weights to them, sums them up, and passes the result through an activation function to produce a binary output (0 or 1).</p>

                <h3>Binary Classification</h3>
                <p>Binary classification is a task where the goal is to classify inputs into one of two possible classes (e.g., yes/no, true/false, spam/ham). Perceptrons are well-suited for this because their output is binary, making them ideal for distinguishing between two types of objects.</p>

                <h3>Structure</h3>
                <ul>
                    <li><strong>Inputs:</strong> The perceptron takes multiple input features (e.g., attributes of an object).</li>
                    <li><strong>Weights:</strong> Each input feature is assigned a weight, which represents the importance of that feature in making the classification decision.</li>
                    <li><strong>Summation:</strong> The weighted inputs are summed together.</li>
                    <li><strong>Activation Function:</strong> The sum is passed through an activation function. For perceptrons, a common activation function is the step function, which outputs 1 if the sum is above a certain threshold and 0 otherwise.</li>
                </ul>

                <h3>Training</h3>
                <p>Perceptrons are trained using a simple learning rule: adjust the weights based on the difference between the predicted output and the actual output (label). The adjustment is made in the direction that reduces the error, allowing the perceptron to learn from its mistakes.</p>

                <h3>Example</h3>
                <p>Consider a perceptron designed to classify whether an email is spam or not based on two input features: the presence of certain keywords and the frequency of those keywords.</p>
                <ul>
                    <li><strong>Inputs:</strong>
                        <ul>
                            <li>\(x_1\): Presence of keywords (1 if present, 0 if not).</li>
                            <li>\(x_2\): Frequency of keywords (a numerical value).</li>
                        </ul>
                    </li>
                    <li><strong>Weights:</strong>
                        <ul>
                            <li>\(w_1\): Weight for the presence of keywords.</li>
                            <li>\(w_2\): Weight for the frequency of keywords.</li>
                        </ul>
                    </li>
                    <li><strong>Summation and Activation:</strong>
                        <ul>
                            <li>The perceptron calculates the weighted sum: \(z = w_1 \cdot x_1 + w_2 \cdot x_2\).</li>
                            <li>This sum is passed through a step function to determine the output:
                                \[
                                \text{output} =
                                \begin{cases}
                                1 & \text{if } z > \text{threshold} \\
                                0 & \text{otherwise}
                                \end{cases}
                                \]
                            </li>
                        </ul>
                    </li>
                    <li><strong>Training:</strong>
                        <ul>
                            <li>During training, the perceptron adjusts \(w_1\) and \(w_2\) to minimize classification errors based on labeled training data (emails labeled as spam or not spam).</li>
                        </ul>
                    </li>
                </ul>

                <h3>Limitations</h3>
                <ul>
                    <li><strong>Linearity:</strong> Perceptrons can only solve linearly separable problems, where a single linear boundary can separate the classes. For more complex problems, where data is not linearly separable, a single perceptron is insufficient.</li>
                    <li><strong>Complexity:</strong> Perceptrons are limited to binary classification tasks and cannot handle multi-class classification or regression tasks directly.</li>
                </ul>
            </section>
        </section>
    </main>
    <footer class="bg-gray-800 text-white p-4 text-center">
    </footer>
</body>

</html>
