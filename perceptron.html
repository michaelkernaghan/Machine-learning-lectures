<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="https://cdn.tailwindcss.com" rel="stylesheet">
    <link rel="stylesheet" href="styles.css">
    <title>Understanding the Perceptron</title>

    <style>
        table {
            width: 100%;
            border-collapse: collapse;
        }

        th,
        td {
            border: 1px solid black;
            padding: 8px;
            text-align: left;
        }

        th {
            background-color: #f2f2f2;
        }

        img {
            width: 100%;
            height: auto;
            margin-top: 20px;
        }

        pre {
            background-color: #f4f4f4;
            border: 1px solid #ddd;
            border-left: 3px solid #f36d33;
            color: #666;
            page-break-inside: avoid;
            font-family: monospace;
            font-size: 15px;
            line-height: 1.6;
            margin-bottom: 1.6em;
            max-width: 100%;
            overflow: auto;
            padding: 1em 1.5em;
            display: block;
            word-wrap: break-word;
        }

        code {
            color: #555;
            word-break: normal;
        }
    </style>
</head>

<body>
    <header class="bg-gray-800 text-white p-4">
        <nav>
            <ul class="flex space-x-4">
                <li><a href="index.html" class="text-blue-400">Home</a></li>
                <li><a href="perceptron.html" class="text-blue-400">Perceptron</a></li>
                <li><a href="applications.html" class="text-blue-400">Applications</a></li>
            </ul>
        </nav>
        <h1>Understanding the Perceptron</h1>
    </header>
    <main class="p-4">
        <section>
            <h2>Introduction to Perceptrons</h2>
            <p>Perceptrons are the simplest type of artificial neural network, often used as a stepping stone to
                understand more complex network structures. This section delves into the perceptron's basics, its
                architecture, and its role as a linear classifier.</p>
            <img src="/images/perceptron.png" alt="Perceptron Architecture">

            <h3>Architecture of a Perceptron</h3>
            <p>The perceptron consists of input nodes (or neurons) connected to a single output node via weighted edges.
                The output node applies an activation function to the weighted sum of the inputs to produce the output.
            </p>

            <img src="/images/perceptron2.png" alt="Perceptron Training Process">
            <h3>Training a Perceptron</h3>
            <p>Training a perceptron involves adjusting the weights based on the errors made by the model during the
                training process. This is typically done using the perceptron learning rule, which is a type of online
                learning.</p>

            <img src="/images/perceptron3.png" alt="Perceptron Example">
            <h3>Example Applications of Perceptrons</h3>
            <p>Initially, perceptrons were used for simple binary classification tasks, such as distinguishing between
                two types of objects based on input features.</p>
        </section>
        <section>
            <h1>Super Simple Neural Network</h1>
            <pre><code>from numpy import exp, array, random, dot

                # Setting up the training data
                # Here we have 4 training samples, each with 3 input features.
                # These inputs can be thought of as features in a simple binary classification problem.
                training_set_inputs = array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]])
                
                # The expected outputs corresponding to the inputs above.
                # It's a column vector (due to the .T transpose operation), suitable for matrix multiplication with input features.
                training_set_outputs = array([[0, 1, 1, 0]]).T
                
                # Seed the random number generator to make the results reproducible.
                # The seed ensures that the random numbers generated are the same every time the program is run.
                random.seed(1)
                
                # Initialize synaptic weights with random values.
                # Weights are initialized randomly but are scaled to range between -1 and 1.
                # There are 3 weights for 3 input features, shaped as a 3x1 matrix.
                synaptic_weights = 2 * random.random((3, 1)) - 1
                
                # Train the neural network using a sigmoid activation function
                # Loop over 10,000 iterations to perform the gradient descent optimization.
                for iteration in range(10000):
                    # Calculate the output of the neural network using the sigmoid activation function.
                    # `dot` is used for matrix multiplication between inputs and weights.
                    # The sigmoid function transforms the dot product into a probability-like output between 0 and 1.
                    output = 1 / (1 + exp(-(dot(training_set_inputs, synaptic_weights))))
                
                    # Adjust synaptic weights based on the difference between actual and predicted outputs.
                    # The adjustment is the derivative of the sigmoid function times the error in output.
                    # dot(training_set_inputs.T, ...) performs gradient descent using the inputs as the derivatives.
                    synaptic_weights += dot(training_set_inputs.T, (training_set_outputs - output) * output * (1 - output))
                
                # Testing the neural network with a new input after training
                # This tests how the trained model predicts a new input.
                test_output = 1 / (1 + exp(-(dot(array([1, 0, 0]), synaptic_weights))))
                print(test_output)  # Print the output of the neural network for the new input.
                </code></pre>
        </section>
        <section>
            <h1>Simple Neural Network</h1>
            <pre><code>from numpy import exp, array, random, dot


                class NeuralNetwork():
                    def __init__(self):
                        # Seed the random number generator, so it generates the same numbers
                        # every time the program runs.
                        random.seed(1)
                
                        # We model a single neuron, with 3 input connections and 1 output connection.
                        # We assign random weights to a 3 x 1 matrix, with values in the range -1 to 1
                        # and mean 0.
                        self.synaptic_weights = 2 * random.random((3, 1)) - 1
                
                    # The Sigmoid function, which describes an S shaped curve.
                    # We pass the weighted sum of the inputs through this function to
                    # normalise them between 0 and 1.
                    def __sigmoid(self, x):
                        return 1 / (1 + exp(-x))
                
                    # The derivative of the Sigmoid function.
                    # This is the gradient of the Sigmoid curve.
                    # It indicates how confident we are about the existing weight.
                    def __sigmoid_derivative(self, x):
                        return x * (1 - x)
                
                    # We train the neural network through a process of trial and error.
                    # Adjusting the synaptic weights each time.
                    def train(self, training_set_inputs, training_set_outputs, number_of_training_iterations):
                        for iteration in range(number_of_training_iterations):
                            # Pass the training set through our neural network (a single neuron).
                            output = self.think(training_set_inputs)
                
                            # Calculate the error (The difference between the desired output
                            # and the predicted output).
                            error = training_set_outputs - output
                
                            # Multiply the error by the input and again by the gradient of the Sigmoid curve.
                            # This means less confident weights are adjusted more.
                            # This means inputs, which are zero, do not cause changes to the weights.
                            adjustment = dot(training_set_inputs.T, error * self.__sigmoid_derivative(output))
                
                            # Adjust the weights.
                            self.synaptic_weights += adjustment
                
                    # The neural network thinks.
                    def think(self, inputs):
                        # Pass inputs through our neural network (our single neuron).
                        return self.__sigmoid(dot(inputs, self.synaptic_weights))
                
                
                if __name__ == "__main__":
                
                    #Intialise a single neuron neural network.
                    neural_network = NeuralNetwork()
                
                    print("Random starting synaptic weights: ")
                    print(neural_network.synaptic_weights)
                
                    # The training set. We have 4 examples, each consisting of 3 input values
                    # and 1 output value.
                    training_set_inputs = array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]])
                    training_set_outputs = array([[0, 1, 1, 0]]).T
                
                    # Train the neural network using a training set.
                    # Do it 10,000 times and make small adjustments each time.
                    neural_network.train(training_set_inputs, training_set_outputs, 100000)
                
                    print("New synaptic weights after training: ")
                    print(neural_network.synaptic_weights)
                
                    # Test the neural network with a new situation.
                    print("Considering new situation [1, 0, 0] -> ?: ")
                    print(neural_network.think(array([0, 0, 0])))
                    </code></pre>
        </section>
    </main>
    <footer class="bg-gray-800 text-white p-4 text-center">

    </footer>
</body>

</html>