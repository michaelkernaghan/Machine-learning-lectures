<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Understanding Gradient Descent</title>
    <link href="https://cdn.tailwindcss.com" rel="stylesheet">
    <link rel="stylesheet" href="styles.css">
    <style>
        body { font-family: Arial, sans-serif; padding: 20px; }
        .section { margin-bottom: 40px; }
        h2 { color: #333; }
        p { margin: 10px 0; }
        img { max-width: 100%; height: auto; }
    </style>
</head>
<body>
    <header class="bg-gray-800 text-white p-4">
        <nav>
            <ul class="flex space-x-4">
                <li><a href="index.html" class="text-blue-400">Home</a></li>
                <li><a href="lecture1.html" class="text-blue-400">1</a></li>
                <li><a href="lecture2.html" class="text-blue-400">2</a></li>
                <li><a href="lecture3.html" class="text-blue-400">3</a></li>
                <li><a href="lecture4.html" class="text-blue-400">4</a></li>
                <li><a href="lecture5.html" class="text-blue-400">5</a></li>
                <li><a href="lecture6.html" class="text-blue-400">6</a></li>
            </ul>
        </nav>
        <h1>Introduction to Gradient Descent</h1>
    </header>

    <div class="section">
        <h2>What is Gradient Descent?</h2>
        <p>Gradient Descent is a fundamental optimization algorithm used in machine learning to minimize a function by iteratively moving towards the steepest descent as defined by the negative of the gradient.</p>
        <img src="./images/gradient_descent.png" alt="Diagram of Gradient Descent">
    </div>
    <div class="section">
        <h2>How Does Gradient Descent Work?</h2>
        <p>Gradient Descent involves calculating the gradient (derivative) of the loss function, which indicates the direction of the steepest ascent. By moving in the opposite direction, we can find the minimum of the function.</p>
        <ul>
            <li>Identify the function to minimize (e.g., loss function in machine learning).</li>
            <li>Compute the gradient of the function.</li>
            <li>Update the parameters of the function in the direction opposite to the gradient.</li>
            <li>Repeat until convergence.</li>
        </ul>
        <img src="./images/gradient_process.png" alt="Process of Gradient Descent">
    </div>
    <div class="section">
        <h2>Types of Gradient Descent</h2>
        <p>There are several variants of Gradient Descent depending on how much data is used to compute the gradient of the objective function.</p>
        <ul>
            <li><strong>Batch Gradient Descent:</strong> Uses the entire dataset to compute the gradient at each iteration.</li>
            <li><strong>Stochastic Gradient Descent:</strong> Uses a single sample from the dataset at each iteration which makes the algorithm much faster and can escape local minima more effectively.</li>
            <li><strong>Mini-batch Gradient Descent:</strong> Uses a subset of the dataset to compute the gradient at each iteration, balancing efficiency and convergence speed.</li>
        </ul>
    </div>
    <div class="section">
        <h2>Applications of Gradient Descent</h2>
        <p>Gradient Descent is used across various fields for optimizing problems, especially in training deep learning models.</p>
        <ul>
            <li>Neural network training</li>
            <li>Large scale machine learning problems</li>
            <li>Anywhere that requires the minimization of error functions</li>
        </ul>
    </div>
</body>
</html>
