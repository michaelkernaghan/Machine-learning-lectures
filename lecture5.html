<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="https://cdn.tailwindcss.com" rel="stylesheet">
    <link rel="stylesheet" href="styles.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']]
            },
            svg: {
                fontCache: 'global'
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
    <title>Lecture 5: Bigrams in Sequence Prediction</title>
</head>
<body>
    <header class="bg-gray-800 text-white p-4">
        <nav>
            <ul class="flex space-x-4">
                <li><a href="index.html" class="text-blue-400">Home</a></li>
                <li><a href="lecture1.html" class="text-blue-400">Lecture 1</a></li>
                <li><a href="lecture2.html" class="text-blue-400">Lecture 2</a></li>
                <li><a href="lecture3.html" class="text-blue-400">Lecture 3</a></li>
                <li><a href="lecture4.html" class="text-blue-400">Lecture 4</a></li>
                <li><a href="lecture6.html" class="text-blue-400">Lecture 6</a></li>
            </ul>
        </nav>
        <h1>Lecture 5: Bigrams in Sequence Prediction</h1>
    </header>
        <main class="p-4">
            <section>
                <h2>Introduction to Bigrams</h2>
                <p>Bigrams can be used to predict elements in a sequence, particularly in the context of language processing. A <strong>bigram</strong> is a sequence of two adjacent elements from a string of tokens, which can be letters, syllables, or words. Bigrams are a basic form of n-gram, where "n" refers to the number of combined elements.</p>
            </section>
            <section>
                <h2>Bigrams in Language Processing</h2>
                <h3>Statistical Language Models</h3>
                <ul>
                    <li><strong>Probability Estimation:</strong> A bigram model predicts the likelihood of a word given the previous word, estimating the probability of any given word sequence.</li>
                    <li><strong>Predictive Text:</strong> Bigram models are straightforward yet powerful tools for tasks like predictive text input.</li>
                </ul>
            </section>
            <section>
                <h3>Example Calculation</h3>
                <p>To calculate the probability of a word following another, use the formula 
                \( P(W_n | W_{n-1}) = \frac{\text{Count}(W_{n-1}, W_n)}{\text{Count}(W_{n-1})} \), 
                where \( W_n \) is the current word and \( W_{n-1} \) is the previous word.</p>
            </section>
            <section>
                <h2>Bigrams in Other Sequences</h2>
                <ul>
                    <li><strong>Music Generation:</strong> In music, bigrams can model the probability of one note following another.</li>
                    <li><strong>Bioinformatics:</strong> In genetic sequences, bigrams could help predict the likelihood of nucleotide sequences.</li>
                    <li><strong>Behavioral Sequences:</strong> Patterns in behavior or movement could potentially be predicted with bigram analysis.</li>
                </ul>
            </section>
            <section>
                <h2>Limitations</h2>
                <ul>
                    <li><strong>Context Ignorance:</strong> Bigrams consider only immediate predecessors, ignoring the broader context.</li>
                    <li><strong>Sparsity:</strong> Many possible bigrams may never occur in the training data, leading to zero probabilities without smoothing techniques.</li>
                    <li><strong>Simplicity:</strong> For complex patterns, higher-order n-grams or more complex models might be more effective.</li>
                </ul>
            </section>
            <section>
                <h2>Conclusion</h2>
                <p>Bigrams are a fundamental tool in predictive modeling for sequences, particularly effective in text-related tasks within natural language processing. They provide a balance between simplicity and effectiveness, making them a popular choice in situations where complex dependencies are less critical, or computational resources are limited. However, for tasks requiring a deeper understanding of context or longer-range dependencies, more sophisticated approaches may be required.</p>
            </section>
        </main>
        <footer class="bg-gray-800 text-white p-4 text-center">
            Â© 2024 Standard Testing <a href="http://standardtesting.io" class="text-blue-400 hover:text-blue-700" target="_blank">standardtesting.io</a>
        </footer>
    </body>
    </html>
    
