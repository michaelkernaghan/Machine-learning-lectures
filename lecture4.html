<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="https://cdn.tailwindcss.com" rel="stylesheet">
    <link rel="stylesheet" href="styles.css">
    <title>Lecture 4: Machine Learning for Sequence Prediction</title>
</head>
<body>
    <header class="bg-gray-800 text-white p-4">
        <nav>
            <ul class="flex space-x-4">
                <li><a href="index.html" class="text-blue-400">Home</a></li>
                <li><a href="lecture1.html" class="text-blue-400">Lecture 1</a></li>
                <li><a href="lecture2.html" class="text-blue-400">Lecture 2</a></li>
                <li><a href="lecture3.html" class="text-blue-400">Lecture 3</a></li>
                <li><a href="lecture5.html" class="text-blue-400">Lecture 5</a></li>
                <li><a href="lecture6.html" class="text-blue-400">Lecture 6</a></li>
            </ul>
        </nav>
        <h1>Lecture 4: Machine Learning for Sequence Prediction</h1>
    </header>
    <main class="p-4">
        <section>
            <h2>Machine Learning for Sequence Prediction</h2>
            <h3>1. Problem Framing:</h3>
            <ul>
                <li><strong>Numerical Sequence Prediction:</strong> The task is to predict the next number in a series based on previous numbers. 
                    Commonly, this involves recognizing patterns or relationships in the sequence, which can be linear, polynomial, or more complex (like trigonometric or exponential).</li>
                <li><strong>Text Sequence Prediction (Next Word/Character):</strong> This involves predicting the next word or character in a text sequence. 
                    The challenge is to understand the context, grammar, and semantics of the text to make accurate predictions.</li>
            </ul>
            <h3>2. Data Representation:</h3>
            <ul>
                <li><strong>Numerical Sequences:</strong> Data can be a simple list of numbers. Data can be a simple list of numbers. These numbers could represent time-series data, counts, measurements, or other quantitative attributes.</li>
                <li><strong>Text Sequences:</strong> Data must be tokenized into words or characters. Each token is typically converted into vectors using methods like one-hot encoding, embeddings, or other numerical representations that capture the semantic properties of the text.</li>
            </ul>
            <h3>3. Model Selection:</h3>
            <ul>
                <li><strong>Numerical Sequence Models:</strong> Might use ARIMA, SVM, regression models, or neural networks.</li>
                <li><strong>Text Sequence Models:</strong> Often involve complex models like RNNs, LSTMs, GRUs, or transformers.</li>
            </ul>
            <h3>4. Training the Model:</h3>
            <p>Both types of sequence prediction involve training a model on a set of known sequence data, then testing it on unseen data. The model learns to predict the next item in the sequence based on the patterns it has learned during training.</p>
            <h3>5. Evaluation:</h3>
            <ul>
                <li><strong>Numerical Sequence Prediction:</strong>  Evaluated on metrics like Mean Squared Error (MSE), Mean Absolute Error (MAE), or more complex metrics tailored to specific types of data or applications.</li>
                <li><strong>Text Sequence Prediction:</strong> Evaluated based on accuracy, perplexity, or using NLP-specific metrics like BLEU scores (for tasks like translation).</li>
            </ul>
            <h3>Similarities and Differences:</h3>
            <ul>
                <li><strong>Similarities:</strong> Both tasks involve learning from sequences, modeling dependencies, and often use RNNs.</li>
                <li><strong>Differences:</strong> Data type, complexity, context, and output space vary significantly between numerical and text data.</li>
            </ul>
        </section>
        <section>
            <p>While the underlying principles of sequence prediction in machine learning are similar, the specifics of application vary between tasks like numerical and text sequence prediction. Each type of data requires tailored approaches in terms of data preprocessing, model selection, and evaluation metrics.</p>
        </section>
        <section>
            <h1>Dog Name Generator Using Bigrams</h1>
            <p>This webpage explains a Python script that generates dog names based on the concept of bigrams. Bigrams are pairs of consecutive characters used to predict the next character in a sequence.</p>
           <p>The code is hosted at: </p>
           <a href="https://github.com/michaelkernaghan/Machine-learning-lectures/blob/main/scripts/dog_name_predictor.py" class="text-blue-400 hover:text-blue-700" target="_blank">dog_name_predictor.py</a> 
           <h2>Python Script Components Explained</h2>
            <h3>1. Importing Libraries</h3>
            <p>The script starts by importing necessary Python libraries:</p>
            <ul>
                <li><code>random</code>: For random selections.</li>
                <li><code>json</code>: To load JSON files containing dog names.</li>
                <li><code>collections.defaultdict</code> and <code>collections.Counter</code>: For managing the bigram model easily.</li>
            </ul>
        
            <h3>2. Loading Dog Names</h3>
            <p>The function <code>load_dog_names</code> reads names from a JSON file:</p>
            <pre><code>def load_dog_names(filename):
            with open(filename, 'r') as file:
                data = json.load(file)
            return data</code></pre>
            <p>This function opens a JSON file, loads the names into a list, and returns this list.</p>
        
            <h3>3. Creating Bigrams</h3>
            <p>The <code>create_bigrams</code> function generates bigrams from the list of names:</p>
            <pre><code>def create_bigrams(names):
            bigrams = []
            for name in names:
                name = "*" + name + "#"
                for i in range(len(name) - 1):
                    bigrams.append((name[i], name[i+1]))
            return bigrams</code></pre>
            <p>It adds a special character at the beginning and end of each name to signify boundaries, then pairs adjacent characters.</p>
        
            <h3>4. Building the Bigram Model</h3>
            <p>The <code>build_bigram_model</code> constructs a probabilistic model from bigrams:</p>
            <pre><code>def build_bigram_model(bigrams):
            model = defaultdict(Counter)
            for bigram in bigrams:
                model[bigram[0]][bigram[1]] += 1
            return model</code></pre>
            <p>This model uses the frequency of each bigram to predict the likelihood of the next character.</p>
        
            <h3>5. Generating a Dog Name</h3>
            <p>The <code>generate_name</code> function creates a new dog name using the bigram model:</p>
            <pre><code>def generate_name(model, max_length=12):
            name = '*'
            while True:
                next_char = random.choices(list(model[name[-1]].keys()), weights=model[name[-1]].values())[0]
                if next_char == '#' or len(name) - 1 == max_length:
                    break
                name += next_char
            return name[1:]</code></pre>
            <p>It begins with a starting character and iteratively selects the next character based on the model until it reaches a stopping condition.</p>
        </section>
    </main>
    <footer class="bg-gray-800 text-white p-4 text-center">
        Â© 2024 Standard Testing <a href="http://standardtesting.io" class="text-blue-400 hover:text-blue-700" target="_blank">standardtesting.io</a>
    </footer>
</body>
</html>
